* spark
** About
*** architecture
**** Driver (spark context)

- Spark Context
  - Driver Program
    - Context
  - Cluster Manager

**** Worker Node (executor)

- Worker Node
  - Executor
    - Tasks

*** Spark components

| DataFrames                             |
| SQL        | Streaming | MLib | Graphx |
| Spark Core                             |
|            |           |      |        |

**** spark core
**** spark sql
**** Spark Streaming
**** MLib
**** Graphx
**** Dataframes
*** Spark abstractions
**** RDDs (Resilient Distributed Datasets)
- Transoformations generated as DAG (Directed Acyclic Graphs)
- DAGs can be recomputed during failiure
- Transformations
  - Map
  - Filter
  - flatMap
  - textFile
  - ...
- Immutable
- 
**** DataFrames
**** DataSets
**** DStreams
*** Lifecycle in spark
Data Sources => Transformation => Actions => UI Dashboard (Real time)/Processed Data
- Load data to cluster
- Create RDD
- Do Transformation
- Perform Action
- Create DataFrame
- Perform Queries on Data Frame
- Run SQL on DATA Frames

** setup hdfs

*** vi /opt/hadoop-2.7.3/etc/hadoop/core-site.xml
#+BEGIN_SRC 
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://node-master:9000</value>
        </property>
    </configuration>

#+END_SRC


*** vi /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml

#+BEGIN_SRC 
<configuration>
    <property>
            <name>dfs.namenode.name.dir</name>
            <value>/home/hadoop/data/nameNode</value>
    </property>

    <property>
            <name>dfs.datanode.data.dir</name>
            <value>/home/hadoop/data/dataNode</value>
    </property>

    <property>
            <name>dfs.replication</name>
            <value>1</value>
    </property>
</configuration>

#+END_SRC

*** vi /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml
 cp /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml.template /opt/hadoop-2.7.3/etc/hadoop/mapred-site.xml

#+BEGIN_SRC 
<configuration>
    <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
    </property>
</configuration>

#+END_SRC

*** vi /opt/hadoop-2.7.3/etc/hadoop/yarn-site.xml
#+BEGIN_SRC 
    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>localhost</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>

#+END_SRC

*** format the namenode and start the services
#+BEGIN_SRC 
hadoop namenode -format
start-all.sh

or 
hadoop namenode -format
start-dfs.sh
start-yarn.sh

#+END_SRC
** install spark

*** Unzip the binaries and set the

**** download the native lib
from: http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
get the link
#+BEGIN_SRC 
wget http://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
#+END_SRC

**** vi /opt/hadoop-2.7.3/etc/hadoop/hadoop-env.sh

set the JAVA_HOME=/usr/java/default/jre

or 
JAVA_HOME=/usr/java/jdk1.8.0_181-amd64/jre

**** vi ~/.bash_profile

#+BEGIN_SRC 
# .bash_profile

# Get the aliases and functions
if [ -f ~/.bashrc ]; then
        . ~/.bashrc
fi

# User specific environment and startup programs

PATH=$PATH:$HOME/.local/bin:$HOME/bin
export JAVA_HOME=/usr/java/default
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
#export HADOOP_HOME=/opt/hadoop-2.8.3
export HADOOP_HOME=/opt/hadoop-2.7.3
export HIVE_HOME=/opt/apache-hive-2.3.3
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export PATH=$PATH:$HIVE_HOME/bin
#export CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib/*:.
#export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib/*:.
export DERBY_HOME=/opt/derby
export PATH=$PATH:$DERBY_HOME/bin
#Apache Hive
#18
export CLASSPATH=$CLASSPATH:$DERBY_HOME/lib/derby.jar:$DERBY_HOME/lib/derbytools.jar
#export HADOOP_CONF_DIR=/opt/hadoop-2.8.3/etc/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_HOME=/opt/spark-2.3.0-bin-hadoop2.7
export PATH=$SPARK_HOME/bin:$PATH

export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
export HADOOP_OPTS="${HADOOP_OPTS}-Djava.library.path=$HADOOP_HOME/lib/native"

export PATH=/opt/python-3.6.2/bin:$PATH
export PYSPARK_SUBMIT_ARGS="pyspark-shell"
export PYSPARK_DRIVER_PYTHON=ipython
export PYSPARK_DRIVER_PYTHON_OPTS='notebook' pyspark

#+END_SRC

**** Spark is super picky about the hostnames and IP(s) . set them right in spark-env.sh

vi /opt/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh

#+BEGIN_SRC 

SPARK_MASTER_HOST=localhost
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8091

#+END_SRC

**** start the hdfs
ensure vi /opt/hadoop-2.7.3/etc/hadoop/hadoop-env.sh contains the JAVA_HOME
#+BEGIN_SRC 
start-all.sh

#+END_SRC

Try running spark shell
#+BEGIN_SRC 
spark-shell --master yarn --deploy-mode client
#+END_SRC

**** Now start the spark master and slave
#+BEGIN_SRC
start-master.sh

start-slave.sh spark://localhost:7077
tail -n230 -f /opt/spark-2.3.0-bin-hadoop2.7/logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-x220.centos.out
#+END_SRC

**** if you are facing the spark-libs.jar not found issue, you will need to pack all the jars in $SPARK_HOME/jars to one

You could also use the spark.yarn.archive option and set that to the location of an archive (you create) containing all the JARs in the $SPARK_HOME/jars/ folder, at the root level of the archive. For example:

- Create the archive: 
~jar cv0f spark-libs.jar -C $SPARK_HOME/jars/ .~
- Upload to HDFS: 
~hdfs dfs -put spark-libs.jar /some/path/.~
- 2a. For a large cluster, increase the replication count of the Spark archive so that you reduce the amount of times a NodeManager will do a remote copy. hdfs dfs ¡Vsetrep -w 10 hdfs:///some/path/spark-libs.jar (Change the amount of replicas proportional to the number of total NodeManagers)
- Set spark.yarn.archive to hdfs:///some/path/spark-libs.jar

**** vi vi /opt/spark-2.3.0-bin-hadoop2.7/conf/spark-defaults.conf

#+BEGIN_SRC 
spark.master.ui.port 8090
spark.master yarn
spark.driver.memory 512m
spark.yarn.am.memory 512m
spark.executor.memory 512m

spark.eventLog.enabled  true
spark.eventLog.dir hdfs://127.0.0.1:9000/spark-logs
spark.history.provider org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory hdfs://127.0.0.1:9000/spark-logs
spark.history.fs.update.interval 10s
spark.history.ui.port 18080
spark.yarn.archive=hdfs:///var/lib/spark/spark-libs.jar
#+END_SRC


#+BEGIN_SRC 
hdfs dfs -mkdir /var
hdfs dfs -mkdir /var/lib
hdfs dfs -mkdir /var/lib/spark
hdfs dfs -put spark-libs.jar /var/lib/spark/.
hdfs dfs -ls /var/lib/spark
#+END_SRC

**** there is an error - namenode is in safemode
#+BEGIN_SRC 
hdfs dfsadmin -safemode leave
#+END_SRC

minReplication

** install jupyter

*** local install python3 and jupyter
#+BEGIN_SRC 
# To allow for building python ssl libs
yum install openssl-devel
# Download the source of *any* python version
cd /usr/src
wget https://www.python.org/ftp/python/3.6.2/Python-3.6.2.tar.xz
tar xf Python-3.6.2.tar.xz 
cd Python-3.6.2

# Configure the build w/ your installed libraries
./configure

#need to do this because ModuleNotFoundError: No module named '_sqlite3' pysqlite2 errors
yum install -y sqlite-devel
./configure --enable-loadable-sqlite-extensions

#make it
make

# Install into /usr/local/bin/python3.6, don't overwrite global python bin
make install

#this is required for pip . pip3 is default configured with ssl
yum install openssl-devel

#now install jupyter notebook
yum install -y jupyter

#+END_SRC
Now running the jupyter notebook (as different user) will only allow you on localhost
su hadoop
jupyter notebook

but to allow it from network here's what u got to do
#+BEGIN_SRC 
jupyter notebook --generate-config

vi /home/hadoop/.jupyter/jupyter_notebook_config.py

#run this command to create the password for you notebook
jupyter notebook password

jupyter notebook password
#Enter password:  ****
#Verify password: ****
#[NotebookPasswordApp] Wrote hashed password to /Users/you/.jupyter/jupyter_notebook_config.json

#+END_SRC

Now you can run your notebook with 

~jupyter notebook~

**** Jupyter notebook permission issue

~unset XDG_RUNTIME_DIR~

YOu can also prepare the hashed password
#+BEGIN_SRC 
In [1]: from notebook.auth import passwd
In [2]: passwd()
Enter password:
Verify password:
Out[2]: 'sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed'

#+END_SRC
Update this:

c.NotebookApp.password = u'sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed'

*** Debian Install jupyter
#+BEGIN_SRC 
sudo apt-get install -y python-dev
sudo pip install --upgrade pip
sudo pip install jupyter
sudo apt-get install -y python-seaborn python-pandas
sudo apt-get install -y ttf-bitstream-vera

#+END_SRC

Run with the command jupyter notebook

** submit job to spark

*** Submit to standalone spark in client mode
#+BEGIN_SRC 
spark-submit --class org.apache.spark.examples.SparkPi --master spark://ip-172-31-30-47.ap-southeast-1.compute.internal:7077 --executor-memory 512m --executor-cores 1 --num-executors 1 --driver-memory 512m --deploy-mode client /opt/cloudera/parcels/CDH/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.16.1-hadoop2.6.0-cdh5.16.1.jar 10
#+END_SRC

*** Submit to yarn
#+BEGIN_SRC 
spark-submit --class org.apache.spark.examples.SparkPi --master yarn --executor-memory 512m --executor-cores 1 --num-executors 1 --driver-memory 512m --deploy-mode client /opt/cloudera/parcels/CDH/lib/spark/examples/lib/spark-examples-1.6.0-cdh5.16.1-hadoop2.6.0-cdh5.16.1.jar 10
#+END_SRC

*** Submit to standalone in cluster mode
- deploy-mode cluster
- spark cluster port can be found from spark webui it is node:6066

#+BEGIN_SRC 
spark-submit --class org.apache.spark.examples.SparkPi --master spark://ip-172-31-30-47.ap-southeast-1.compute.internal:6066 --executor-memory 512m --executor-cores 1 --num-executors 1 --driver-memory 512m --deploy-mode cluster $(pwd)/spark-examples-1.6.0-cdh5.16.1-hadoop2.6.0-cdh5.16.1.jar 10
#+END_SRC
** start spark standalone

*** open up the ports
firewall-cmd --zone=public --add-port=7077/tcp --permanent
firewall-cmd --zone=public --add-port=8090/tcp --permanent
systemctl restart firewalld

*** update the spark conf
vi /opt/spark-2.3.0-bin-hadoop2.7/conf/spark-defaults.conf

- here we set 
spark.master.ui.port=8090

*** start the spark master and worker standalone
/opt/spark-2.3.0-bin-hadoop2.7/sbin/start-mater.sh
/opt/spark-2.3.0-bin-hadoop2.7/sbin/start-slave.sh spark://localhost:7077


*** start the spark shell with remote master
.\spark-shell --master spark://192.168.0.119:7077

*** submit a sample job (tbc)

./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://192.168.0.119:7077  examples/jars/spark-examples*.jar 10
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://192.168.0.119:7077 --driver-memory 4g --executor-memory 2g --executor-cores 1 --queue thequeue examples/jars/spark-examples*.jar 10

*** Or on the yarn (tbc):
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://192.168.0.119:7077  examples/jars/spark-examples*.jar 10./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://192.168.0.119:70771  examples/jars/spark-examples*.jar 10./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://192.168.0.119:70771  examples/jars/spark-examples*.jar 10./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://192.168.0.119:70771  examples/jars/spark-examples*.jar 10
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster --driver-memory 4g --executor-memory 2g --executor-cores 1 --queue thequeue examples/jars/spark-examples*.jar 10


** start zookeeper, kafka, spark

*** start zookeeper first

#+BEGIN_SRC 
set ZOOKEEPER_HOME=C:\Apps\Tools\zookeeper
zookeeper/bin/zkServer.cmd
#+END_SRC

*** start kafka

#+BEGIN_SRC 
#start server
kafka/bin/windows/kafka-server-start.bat kafka/config/server.properties

#start topic
.\kafka\bin\windows\kafka-console-producer.bat --broker-list localhost:9092 --topic twittertopic

#console consumer
.\kafka\bin\windows\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic twittertopic

#submit the spark producer


#+END_SRC

*** submit a spark job

If there is hadoop binaries not found make sure to add HADOOP_HOME environment parameters.
If there is a permission issue make sure to download winutils and place in hadoop/bin and then change the ownership of executables using winutil

#+BEGIN_SRC 
set HADOOP_HOME=C:\Apps\Tools\hadoop-2.7.7
#+END_SRC

#+BEGIN_SRC 
C:\Apps\Tools\hadoop-2.7.7\bin\winutils.exe chmod -R +x C:\Apps\Tools\hadoop-2.7.7\bin
#+END_SRC


#+BEGIN_SRC 
.\spark\bin\spark-submit D:\Amit\projects\amitthk\bitbucket\pysparktest\kafka_tweet_consumer.py localhost 9092 twittertopic
#+END_SRC

#+BEGIN_SRC 
.\spark\bin\spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0 --class com.innda
ta.StructuredStreaming.Kafka --master local[*] D:\Amit\projects\amitthk\bitbucket\pysparktest\kafka_tweet_consumer.py lo
calhost 9092 twittertopic
#+END_SRC

** spark shell
#+BEGIN_SRC 
sc
help(sc)
sc.serializer
sc.sparkUser()
sc.stop()

test_rdd = sc.emptyRDD() //wont work as sc stopped above
sc = SparkContext.getOrCreate() //brand new SC



#+END_SRC

- RDD
  - Resilient - if it fails make it work
  - Distributed /partitioned
  - Dataset
- Five properties of RDD:
  - Partitions
  - Dependencies
  - Functions to compute partitions
  - Partitiner (key/value RDDs - optional)
  - Preferred locations fo compute - optional)
- PairRDD
  - Tupeles
  - Good for grouping /aggregating
  - 
- Creating RDDs
  - parallelize
  - sc.parallelize(....)
  - list.getNumPartitions()

*** spark configuration
- Properties
application params
- Environment vars
system specific
- logging
log4j.properties

Application (in code)

> 
flags passed in spark2-submit /spark2-shell
>

spark-defaults.conf


** pyspark codes

*** simple netcat reader

Lets say we write the following program to read from netcat and show word count

We open up the netcat terminal and start sending some text on port 8099
#+BEGIN_SRC 
nc -l 8099
<enter some text to send>
#+END_SRC


#+BEGIN_SRC 
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode
from pyspark.sql.functions import split

if __name__ == '__main__':
    if len(sys.argv) !=3:
        print('insufficient params')
        #exit(-1)
    if(len(sys.argv) > 1 and sys.argv[1] is not None):
        host = sys.argv[1]
    else:
        host = 'localhost'

    if(len(sys.argv) > 1 and sys.argv[2] is not None):
        port = int(sys.argv[2])
    else:
        port = 8099

    spark = SparkSession.builder.appName("Spark Stream 1").getOrCreate()
    spark.sparkContext.setLogLevel('ERROR')

    lines = spark.readStream.format('socket').option('host',host).option('port', port).load()

    words = lines.select(explode(split(lines.value, ' ')).alias('word'))
    wordCounts = words.groupBy('word').count()

    query = wordCounts.writeStream.outputMode('complete').format('console').start()

    query.awaitTermination()

#+END_SRC

*** simple directory wather

Lets run a container with logs routed to a location

#+BEGIN_SRC 

#+END_SRC

We route the ps logs to a log in /var/log
#+BEGIN_SRC 
while true; do ps -elf --no-headers >> /var/log/ps.log ;sleep 5; done
#+END_SRC

#+BEGIN_SRC 
from pyspark.sql.types import *
from pyspark.sql import SparkSession


if __name__ == "__main__":
    sparkSession = SparkSession.builder.master('local').appName('SparkLogAppendMode').getOrCreate()

    sparkSession.sparkContext.setLogLevel('ERROR')

    schema = StructType([StructField("P", StringType(), True),
                         StructField("S", StringType(), True),
                         StructField("UID", StringType(), True),
                         StructField("PID", StringType(), True),
                         StructField("PPID", StringType(), True),
                         StructField("C", StringType(), True),
                         StructField("PRI", StringType(), True),
                         StructField("NI", StringType(), True),
                         StructField("ADDR", StringType(), True),
                         StructField("SZ", StringType(), True),
                         StructField("WCHAN", StringType(), True),
                         StructField("STIME", StringType(), True),
                         StructField("TTY", StringType(), True),
                         StructField("TIME", StringType(), True),
                         StructField("CMD", StringType(), True)])

    fileStreamDf = sparkSession.readStream.option("header","true")\
        .option("delimiter","\t").schema(schema).csv("D:\\Amit\\projects\\amitthk\\bitbucket\\testsrc\\docker\\all_logs")

    print(" ")
    print("Stream ready?")
    print(fileStreamDf.isStreaming)

    print(" ")
    print("Schema: ")
    print(fileStreamDf.printSchema)

    trimmedDF = fileStreamDf.select(fileStreamDf.TIME, fileStreamDf.CMD)

    query = trimmedDF.writeStream.outputMode("append").format("console").option("truncate", "false").option("numRows", 30).start().awaitTermination()

#+END_SRC

#+BEGIN_SRC 
.\spark-submit D:\Amit\projects\amitthk\bitbucket\pysparktest\dir_log_reader.py
#+END_SRC

*** add timestamp

#+BEGIN_SRC 
from pyspark.sql.types import *
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
import time, datetime


if __name__ == "__main__":
    sparkSession = SparkSession.builder.master('local').appName('SparkLogAppendMode').getOrCreate()

    sparkSession.sparkContext.setLogLevel('ERROR')

    schema = StructType([StructField("P", StringType(), True),
                         StructField("S", StringType(), True),
                         StructField("UID", StringType(), True),
                         StructField("PID", StringType(), True),
                         StructField("PPID", StringType(), True),
                         StructField("C", StringType(), True),
                         StructField("PRI", StringType(), True),
                         StructField("NI", StringType(), True),
                         StructField("ADDR", StringType(), True),
                         StructField("SZ", StringType(), True),
                         StructField("WCHAN", StringType(), True),
                         StructField("STIME", StringType(), True),
                         StructField("TTY", StringType(), True),
                         StructField("TIME", StringType(), True),
                         StructField("CMD", StringType(), True)])

    fileStreamDf = sparkSession.readStream.option("header","true")\
        .option("delimiter"," ").schema(schema).csv("D:\\Amit\\projects\\amitthk\\bitbucket\\pysparktest\\docker\\all_logs")

    def add_timestamp():
        ts = time.time()
        timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
        return timestamp

    print(" ")
    print("Stream ready?")
    print(fileStreamDf.isStreaming)

    print(" ")
    print("Schema: ")
    print(fileStreamDf.printSchema)

    add_timestamp_udf = udf(add_timestamp, StringType())

    tsFileStream = fileStreamDf.withColumn("timestamp", add_timestamp_udf())

    trimmedDF = fileStreamDf.select(fileStreamDf.TIME, fileStreamDf.CMD, "timestamp")

    query = trimmedDF.writeStream.outputMode("append").format("console").option("truncate", "false").option("numRows", 30).start().awaitTermination()

#+END_SRC

*** sql aggregation os spark streams

#+BEGIN_SRC 
from pyspark.sql.types import *
from pyspark.sql import SparkSession


if __name__ == "__main__":
    sparkSession = SparkSession.builder.master('local').appName('SparkLogAppendMode').getOrCreate()

    sparkSession.sparkContext.setLogLevel('ERROR')

    schema = StructType([StructField("P", StringType(), True),
                         StructField("S", StringType(), True),
                         StructField("UID", StringType(), True),
                         StructField("PID", StringType(), True),
                         StructField("PPID", StringType(), True),
                         StructField("C", StringType(), True),
                         StructField("PRI", StringType(), True),
                         StructField("NI", StringType(), True),
                         StructField("ADDR", StringType(), True),
                         StructField("SZ", StringType(), True),
                         StructField("WCHAN", StringType(), True),
                         StructField("STIME", StringType(), True),
                         StructField("TTY", StringType(), True),
                         StructField("TIME", StringType(), True),
                         StructField("CMD", StringType(), True)])

    fileStreamDf = sparkSession.readStream.option("header","true")\
        .option("delimiter"," ").schema(schema).csv("D:\\Amit\\projects\\amitthk\\bitbucket\\pysparktest\\docker\\all_logs")

    print(" ")
    print("Stream ready?")
    print(fileStreamDf.isStreaming)

    print(" ")
    print("Schema: ")
    print(fileStreamDf.printSchema)

    fileStreamDf.createOrReplaceTempView("TempTable")

    trimmedDF = fileStreamDf.select(fileStreamDf.TIME, fileStreamDf.CMD)

    categoryDF = sparkSession.sql("SELECT HOSTNAME, PPID, TIME, CMD from TempTable where CMD = 'spark'")

    psPerServer = categoryDF.groupBy("hostname").agg({"value":"sum"}).withColumnRenamed("sum(value)", "processes").orderBy("HOSTNAME",ascending=false)

    query = trimmedDF.writeStream.outputMode("append").format("console").option("truncate", "false").option("numRows", 30).start().awaitTermination()
#+END_SRC


*** kafka tweets producer

#+BEGIN_SRC 
import sys
import tweepy
from tweepy import OAuthHandler
from tweepy import Stream
from tweepy import StreamListener
import json
import pykafka

class TweetsConsumer(StreamListener):

    def __init__(self, kafkaProducer):
        print("Procuce tweets")
        self.producer = kafkaProducer

    def on_data(self, raw_data):
        try:
            data_json = json.loads(raw_data)
            words = data_json["text"].split()
            lstHashTags = list(filter(lambda x: x.lower().startsWith("#"),words))
        except KeyError as e:
            print("Error in data %s"%str(e))
        return True

    def login_to_twitter(kafkaProducer, tracks):
        api_key = ""
        api_secret = ""

        access_token = ""
        access_token_secret = ""

        auth = OAuthHandler(api_key, api_secret)
        auth.set_access_token(access_token, access_token_secret)

        twitter_stream = Stream(auth, TweetsConsumer(kafkaProducer))
        twitter_stream.filter(tracks=tracks, languages=['en'])

    if __name__ == "__main__":
        if(len(sys.argv)<5):
            print("insufficient args", sys.stderr)
            exit(-1)

        host = sys.argv[1]
        port = sys.argv[2]
        topic = sys.argv[3]
        tracks = sys.argv[4]

        kafkaClient = pykafka.KafkaClient(host+":"+port)
        kafkaProducer = kafkaClient.topics[bytes(topic, "utf-8")].get_producer()
        login_to_twitter(kafkaProducer, tracks)

#+END_SRC

*** kafka tweets consumer

#+BEGIN_SRC 
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

if __name__ == "__main__":
    if (len(sys.argv) < 5):
        print("insufficient args", sys.stderr)

    if(len(sys.argv)>1 and sys.argv[1] is not None):
        host = sys.argv[1]
    else:
        host = 'localhost'

    if (len(sys.argv)>1 and sys.argv[2] is not None):
        port = sys.argv[2]
    else:
        port = '9092'

    if (len(sys.argv)>2 and sys.argv[3] is not None):
        topic = sys.argv[3]
    else:
        topic = 'twittertopic'

    if (len(sys.argv)>3 and sys.argv[4] is not None):
        tracks = sys.argv[4]

    spark = SparkSession.builder.appName("Tweek consumer").getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")

    tweetsDFRaw = spark.readStream.format("kafka").option("kafka.bootstrap.servers", host+":"+port).option("subscribe", topic).load()

    tweetsDF = tweetsDFRaw.selectExpr("CAST(value AS STRING) as tweet").withColumn("tweet")

    query = tweetsDF.writeStream.outputMode("append").format("console").option("truncate", "false").trigger(processingTime="5 seconds").start().awaitTermination()
#+END_SRC
