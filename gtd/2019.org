* GTD
** Ansible DI Jobs
- Get one job done
- Prod Ready
** Spark nomad - build first outside bring in
- Kerberos - setup the kdc
- Create the priniciples and setup the kerberos
- Setup the mysql database for hive server
- setup the users for hive server
- Self signed certificates trust store and jks
- LDAP integrate cluster/ssd
- test
- upload to nexus
- make it work inside
- Docker workloads on NOmad
- DONE
- clone the cloudform hadoop
- install docker
- install nomad
- install consul
- install spark
** Vault POC into prod
- Unseal script test from the shell
- Add to post service up
- Release the latest vault as well as old vault - the the git hashes
- Create the JIRA tickets submit the UAT signoff and CSV
- Reach out to ITSS release team for approval
- Test the local vault with raft awareness
- Copy over the CI from ansiblectr or 
- Setup the CD job in dev jenkins
- Setup the CR for release
- Auto unseal script
- Setup HA
** Ansible ssh bastion for prd
** complete the python35 
** Cluster for test aws
- Jenkinsfile for wget and upload to s3 repo
- s3 repo
- harden role
- set root password on all
- disable sudo
- diff merge the roles
- ssd setup
- test the vault
- test the alluxio ctr
- nomad spark cluster
** Nomad Spark cluster
- Merge the nomad terraform
- Upload nomad package to nexus
- Setup the cluster
- Harden the playbook
** Ansible consumable
- Fix ansible playbook command
- Local repo or download from nexus, fix proxy settings
** Collibra containerization
- Split the images by parameter
- Volume mounting and checking in entrypoint
- Docker compose for deploy
- Connect the services together
** Restauthcl
- Multiple services in one docker
- Complete the mongodb api
- Code generation with template
** Collibra automation
- Ansible set the vmstat and other params with root
- https://docs.ansible.com/ansible/2.4/pam_limits_module.html
- https://stackoverflow.com/questions/38155108/how-to-increase-limit-for-open-processes-and-files-using-ansible
- https://docs.ansible.com/ansible/latest/modules/sysctl_module.html
- You will need to copy both the files. limts in sysctl and PAM. PAM has a module as well
- - hosts: all
-   become: true
-   tasks:
-     - name: configure system settings, file descriptors and number of threads
-       pam_limits:
-         domain: <--your-username-->
-         limit_type: "{{item.limit_type}}"
-         limit_item: "{{item.limit_item}}"
-         value: "{{item.value}}"
-       with_items:
-         - { limit_type: '-', limit_item: 'nofile', value: 65536 }
-         - { limit_type: '-', limit_item: 'nproc', value: 65536 }
-         - { limit_type: 'soft', limit_item: 'memlock', value: unlimited }
-         - { limit_type: 'hard', limit_item: 'memlock', value: unlimited }
-     - name: reload settings from all system configuration files
- shell: sysctl --system
- Check and fix why inventory is not able to connect
- Is the firewall not open between the jenkins n host
- Is the key correctly added. if not, can we add our own key
- Checkout the inventory and deploy collibra
- Containers - split by param
- Deploy with ansible
- Deploy with ansible containers
** Nomad POC
- Get the nomad cluster up
** Ansible DI Job completion
- Convert one of the DI jobs to Ansible
- Run multiple jobs parallel with ansible
** Sparkhadoop CF cluster
- SSSD + Ldap + Kerberos
- https://github.com/hellofresh/ansible-sssd-ldap
- https://github.com/ISU-Ansible/ansible-sssd
- Root & non root playbooks separate
- Create service users and next login with those for install
- SSH Key exchange
** Must do personal sunderies
- Pay the rental
- Pay the starhub and sp services bills
- Pay the income tax
- Update monika's documents
** Projects
- Complete the alluxio consumable
  - Terraform
    - take all the region vpc stuff from ada  depl
    - run the pipeline with custom nodes and tags
    - copy the playbook to dest and run local after login
  - evolve consumable
    - spin up the ec2 with tags only
    - copy the playbook to dest and run local
  - Test the full playbook on localhost
    - Docker tar download and load to docker is completed
    - Download the right alluxio version later
    - One playbook to download the ansible artifact to host
    - Then run the playbook from host locally
    - Inventory should be generated by Terraform
    - Update the terraform to create simple stack
  - Alluxio Cloudformation Docker Playbook
- Pull ansible from cloudform and install
- Run the playbook
- Sagemaker Alluxio ML
  - ADA Analytics 52 nodes ansible playbook setup cdh cluster
  - Complete the CICD of the onboarding dashboard
    - Approval api to be done or not?
    - Complete the Onboarding dashboard
- Local request database mariadb/postgres
- Integration with JIRA
  - Nomad POC
    - REfine the Jenkinsfile and terraform
    - Spin up the cluster with terraform
    - Start the spark first
    - Test spark jobs
    - Plan and install rest of the services to be integrated
  - CDO Governance workflow
  - Collibra terraform + Ansible automation
    - Ansible playbook is done
    - Spawn the cluster with terraform/evolve
  - Collibra containerized split and run
    - Terraform provision, get inventory ec2 list
    - Run playbook localhost
  - Complete up the Ansible ssh PROD POC
    - Connect to the qa server with ssh key
    - Write down the execution plan for CR
    - Get the SSH exchange done do the CR
    - Explore one of the ingestions and get it done
  - Complete the Elasticsearch playbooks
    - Root task for user creation
    - Root task for sysctl updates
    - The tarball solution
    - Nohup run and service file
    - HA config
    - Authentication plugin/Nginx
    - Metricbeat, filebeat and journalbeat to puppet
    - Monitoring Dashboards
- Elasticsearch setup
- Kibana
- Unravel Data
  - EUREKA Zuul POC
    - From CDH invoke and update the eureka service
  - Collibra containerization
    - Check for the volume mount
    - Dissect the deployment to multiple pods deploy separate
  - DevOps CICD code generator
    - J2 template from codesources
    - Jenkinsfile for
      - ansible project
      - simple files
      - python
      - nodejs
      - maven
    - select project name, app code, repos
  - Tableau Server CR for deploy of odbc and setting up dashboards
    - Draft the CR for tableau
    - PROD openshift pipeline to deploy all from Nexus
    - Draft down the firewall stuff
  - Complete the Ansible SSH file upload POC
  - Complete the cdhhadoopclluster
  - Complete the sparkhadoopinstall
  - Complete the ADAC Gateway
    - Get the S3 buckets
    - Finalize the roles and create roles for Internal access
    - Finalize and create roles for vendor access
    - Complete the testing
  - Complete the single node cloudfspark
  - Trifacta POC
    - Finalize and update about the POC success
  - Urgent burning Toolchain pocs
    - Nomad on Spark
    - Openshift Kubernetes
    - Ldap
    - Kafka
    - Glusterfs,, NAS,, NFS,, SAN,, storages,, NAT,
    - wireshark
  - Complete the Elasticsearch playbooks
    - Root task for user creation
    - Root task for sysctl updates
    - The tarball solution
    - Nohup run and service file
    - HA config
    - Authentication plugin/Nginx
  - LDAP and Kerberos setup and completion for cdh
  - Complete the Tableau server automation and learning
    - Set up for the drivers list and get signoff
    - Licenses for users
    - Automation playbook for Tableau installation
  - Spring boot DevOps dashboard
  - Spark Jobs for Metric Collections
  - Business Dashboard project completion
  - Openshift Profile should be used
  - Should docker save as tarball upload to nexus
  - should tarball deployment to nexus
  - In release should download tarball and docker load
  - should untar deployment and run processing from params
  - To check for one of the clusters to deploy to QA first
  - Create CR for deploment get signoffs
